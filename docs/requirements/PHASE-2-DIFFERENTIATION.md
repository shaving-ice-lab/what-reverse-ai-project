# Phase 2: 差异化功能 - 详细需求文档

> **版本**: v1.0  
> **更新日期**: 2026-01-29  
> **预计周期**: 6-8 周  
> **前置条件**: Phase 1 完成
> **优先级**: P0/P1 - 核心差异化

---

## 目录

1. [阶段目标](#1-阶段目标)
2. [本地模式](#2-本地模式)
3. [本地 LLM 集成](#3-本地-llm-集成)
4. [时间旅行调试](#4-时间旅行调试)
5. [AI 辅助构建](#5-ai-辅助构建)
6. [其他功能](#6-其他功能)
7. [技术架构](#7-技术架构)
8. [开发计划](#8-开发计划)

---

## 1. 阶段目标

### 1.1 核心目标

> **建立核心竞争壁垒，与竞品形成本质差异**

差异化定位:
- ✅ **本地优先** — Coze/Manus 无法复制
- ✅ **数据安全** — 企业刚需
- ✅ **调试体验** — 目前无竞品
- ✅ **智能构建** — 降低门槛

### 1.2 用户故事

**US-101: 作为关注数据安全的用户，我想在本地运行工作流**
```gherkin
Given 我下载并安装了 AgentFlow 桌面版
When 我创建一个工作流并运行
Then 所有数据应该只在我的电脑上处理
And 不需要连接互联网（使用本地 LLM 时）
```

**US-102: 作为用户，我想使用本地 LLM**
```gherkin
Given 我已安装 Ollama 并下载了模型
When 我在 LLM 节点中选择本地模型
Then 应该调用本地 Ollama API
And 数据不会发送到任何外部服务
```

**US-103: 作为用户，我想调试失败的工作流**
```gherkin
Given 我的工作流执行失败了
When 我打开时间旅行调试面板
Then 我应该能看到每个节点的输入输出
And 我可以点击任意节点查看当时的状态
And 我可以重新运行某个节点
```

**US-104: 作为新用户，我想快速创建工作流**
```gherkin
Given 我在编辑器中
When 我输入"帮我创建一个获取股票价格并用 AI 分析的工作流"
Then AI 应该自动生成对应的节点和连线
And 我可以在此基础上修改
```

### 1.3 成功标准

| 指标 | 目标 |
|------|------|
| 桌面版安装成功率 | > 95% |
| 本地模式执行成功率 | > 90% |
| Ollama 集成可用性 | 100% |
| AI 生成工作流准确率 | > 70% |
| 公测用户数 | > 500 |

---

## 2. 本地模式

### 2.1 概述

使用 Tauri 构建跨平台桌面应用，实现完全本地运行。

**支持平台**:
- Windows 10+ (x64)
- macOS 12+ (Intel & Apple Silicon)
- Linux (Ubuntu 20.04+, Fedora 35+)

### 2.2 功能需求

#### REQ-201: Tauri 桌面应用

| ID | 功能 | 优先级 | 说明 |
|----|------|--------|------|
| REQ-201-1 | 应用打包 | P0 | Windows/macOS/Linux 安装包 |
| REQ-201-2 | 自动更新 | P1 | 检测并安装更新 |
| REQ-201-3 | 系统托盘 | P2 | 后台运行 |
| REQ-201-4 | 开机启动 | P2 | 可选自启动 |

#### REQ-202: 本地执行引擎

| ID | 功能 | 优先级 | 说明 |
|----|------|--------|------|
| REQ-202-1 | 嵌入式执行引擎 | P0 | Rust/Go 编译进 Tauri |
| REQ-202-2 | 本地数据库 | P0 | SQLite 存储工作流和执行记录 |
| REQ-202-3 | 离线运行 | P0 | 无需网络即可运行（本地 LLM） |
| REQ-202-4 | 资源管理 | P1 | CPU/内存使用监控 |

#### REQ-203: 云端同步（可选）

| ID | 功能 | 优先级 | 说明 |
|----|------|--------|------|
| REQ-203-1 | 工作流同步 | P1 | 本地↔云端双向同步 |
| REQ-203-2 | 端到端加密 | P1 | 同步数据加密 |
| REQ-203-3 | 冲突解决 | P2 | 多设备冲突处理 |
| REQ-203-4 | 选择性同步 | P2 | 用户选择哪些工作流同步 |

### 2.3 架构设计

```
┌─────────────────────────────────────────────────────────────────┐
│                     Tauri 桌面应用架构                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    前端 (WebView)                        │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐     │   │
│  │  │ 工作流编辑器 │  │  设置面板   │  │  调试面板   │     │   │
│  │  │  (React)    │  │  (React)    │  │  (React)    │     │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘     │   │
│  └───────────────────────────┬─────────────────────────────┘   │
│                              │ Tauri IPC                        │
│  ┌───────────────────────────┴─────────────────────────────┐   │
│  │                    后端 (Rust)                           │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐     │   │
│  │  │  执行引擎   │  │  数据管理   │  │  系统集成   │     │   │
│  │  │ (Go WASM?) │  │  (SQLite)   │  │ (OS APIs)   │     │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘     │   │
│  │                        │                                 │   │
│  │           ┌────────────┴────────────┐                   │   │
│  │           │     Ollama 客户端       │                   │   │
│  │           │    (HTTP localhost)     │                   │   │
│  │           └─────────────────────────┘                   │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 2.4 Tauri 命令定义

```rust
// src-tauri/src/commands.rs

#[tauri::command]
async fn execute_workflow(
    workflow_id: String,
    inputs: serde_json::Value,
    app_handle: tauri::AppHandle,
) -> Result<ExecutionResult, String> {
    // 本地执行工作流
}

#[tauri::command]
async fn get_workflows() -> Result<Vec<Workflow>, String> {
    // 从 SQLite 获取工作流列表
}

#[tauri::command]
async fn save_workflow(workflow: Workflow) -> Result<(), String> {
    // 保存工作流到 SQLite
}

#[tauri::command]
async fn check_ollama_status() -> Result<OllamaStatus, String> {
    // 检查 Ollama 是否运行
}

#[tauri::command]
async fn list_local_models() -> Result<Vec<ModelInfo>, String> {
    // 列出已下载的 Ollama 模型
}
```

### 2.5 本地数据库 Schema

```sql
-- SQLite Schema for Desktop App

-- 工作流表
CREATE TABLE workflows (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    description TEXT,
    definition TEXT NOT NULL, -- JSON
    status TEXT DEFAULT 'draft',
    version INTEGER DEFAULT 1,
    cloud_id TEXT, -- 对应的云端 ID（如果同步）
    synced_at TEXT, -- 最后同步时间
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    updated_at TEXT DEFAULT CURRENT_TIMESTAMP
);

-- 执行记录表
CREATE TABLE executions (
    id TEXT PRIMARY KEY,
    workflow_id TEXT NOT NULL,
    status TEXT NOT NULL,
    inputs TEXT, -- JSON
    outputs TEXT, -- JSON
    node_states TEXT, -- JSON
    error_message TEXT,
    started_at TEXT,
    completed_at TEXT,
    duration_ms INTEGER,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (workflow_id) REFERENCES workflows(id)
);

-- 设置表
CREATE TABLE settings (
    key TEXT PRIMARY KEY,
    value TEXT NOT NULL,
    updated_at TEXT DEFAULT CURRENT_TIMESTAMP
);

-- API Keys 表（本地加密存储）
CREATE TABLE api_keys (
    id TEXT PRIMARY KEY,
    provider TEXT NOT NULL,
    name TEXT,
    key_encrypted TEXT NOT NULL,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP
);
```

---

## 3. 本地 LLM 集成

### 3.1 概述

集成 Ollama，支持完全本地运行 LLM。

**支持的模型**:
- Llama 3.1/3.2
- Mistral
- CodeLlama
- Qwen
- 其他 Ollama 支持的模型

### 3.2 功能需求

#### REQ-301: Ollama 集成

| ID | 功能 | 优先级 | 说明 |
|----|------|--------|------|
| REQ-301-1 | 状态检测 | P0 | 检测 Ollama 是否运行 |
| REQ-301-2 | 模型列表 | P0 | 获取已下载的模型 |
| REQ-301-3 | 模型选择 | P0 | LLM 节点可选择本地模型 |
| REQ-301-4 | 模型下载 | P1 | 从应用内下载模型 |
| REQ-301-5 | 模型管理 | P2 | 删除、更新模型 |

#### REQ-302: Ollama API 对接

| ID | 功能 | 优先级 | 说明 |
|----|------|--------|------|
| REQ-302-1 | Chat Completion | P0 | /api/chat 对话接口 |
| REQ-302-2 | 流式输出 | P0 | 支持流式响应 |
| REQ-302-3 | Embeddings | P1 | /api/embeddings 向量 |
| REQ-302-4 | 参数配置 | P0 | temperature, top_p 等 |

### 3.3 UI 设计

**LLM 节点模型选择器**:

```
┌─────────────────────────────────────────────────────────────┐
│ 模型选择                                                     │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─ 云端模型 ──────────────────────────────────────────┐   │
│  │  ○ GPT-4                    需要 API Key            │   │
│  │  ○ GPT-3.5 Turbo            需要 API Key            │   │
│  │  ○ Claude 3 Opus            需要 API Key            │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  ┌─ 本地模型 (Ollama) ─────────────────────────────────┐   │
│  │  ● llama3.1:8b              ✅ 已安装 (4.7GB)       │   │
│  │  ○ llama3.1:70b             ⬇️ 点击下载             │   │
│  │  ○ mistral:7b               ✅ 已安装 (4.1GB)       │   │
│  │  ○ qwen2:7b                 ✅ 已安装 (4.4GB)       │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  ⚠️ Ollama 未运行 [启动 Ollama]                            │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 3.4 Ollama 客户端实现

```typescript
// lib/ollama.ts

interface OllamaClient {
  baseUrl: string; // 默认 http://localhost:11434
  
  // 检查状态
  checkStatus(): Promise<boolean>;
  
  // 列出模型
  listModels(): Promise<Model[]>;
  
  // 对话
  chat(request: ChatRequest): Promise<ChatResponse>;
  
  // 流式对话
  chatStream(request: ChatRequest): AsyncGenerator<ChatChunk>;
  
  // 下载模型
  pullModel(name: string, onProgress: (progress: number) => void): Promise<void>;
}

interface ChatRequest {
  model: string;
  messages: Message[];
  options?: {
    temperature?: number;
    top_p?: number;
    max_tokens?: number;
  };
  stream?: boolean;
}
```

### 3.5 模型推荐

| 用途 | 推荐模型 | 参数量 | 显存需求 | 说明 |
|------|----------|--------|----------|------|
| 通用对话 | llama3.1:8b | 8B | 8GB | 平衡性能和资源 |
| 高质量 | llama3.1:70b | 70B | 48GB+ | 需要高端 GPU |
| 代码 | codellama:13b | 13B | 16GB | 代码生成优化 |
| 中文 | qwen2:7b | 7B | 8GB | 中文能力强 |
| 轻量 | mistral:7b | 7B | 8GB | 速度快 |

---

## 4. 时间旅行调试

### 4.1 概述

提供可视化的执行历史回溯功能，让用户能够查看工作流执行的每一步状态。

### 4.2 功能需求

#### REQ-401: 执行历史记录

| ID | 功能 | 优先级 | 说明 |
|----|------|--------|------|
| REQ-401-1 | 完整状态快照 | P0 | 记录每个节点的输入输出 |
| REQ-401-2 | 时间线视图 | P0 | 按时间顺序展示执行步骤 |
| REQ-401-3 | 节点状态高亮 | P0 | 画布上高亮当前查看的节点 |
| REQ-401-4 | 数据对比 | P1 | 对比不同执行的数据差异 |

#### REQ-402: 调试操作

| ID | 功能 | 优先级 | 说明 |
|----|------|--------|------|
| REQ-402-1 | 步骤回溯 | P0 | 点击时间线回到任意步骤 |
| REQ-402-2 | 单节点重跑 | P1 | 从某个节点重新执行 |
| REQ-402-3 | 输入修改重跑 | P1 | 修改节点输入后重跑 |
| REQ-402-4 | 断点设置 | P2 | 设置断点暂停执行 |

### 4.3 UI 设计

```
┌─────────────────────────────────────────────────────────────────┐
│                     时间旅行调试面板                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  执行 ID: exec_abc123    状态: ✅ 完成    耗时: 15.2s           │
│                                                                 │
│  ┌─ 时间线 ────────────────────────────────────────────────┐   │
│  │                                                          │   │
│  │  10:05:00 ─●─ 开始                                       │   │
│  │            │                                             │   │
│  │  10:05:01 ─●─ HTTP 请求 (获取天气)        ⏱️ 2.1s        │   │
│  │            │  └─ 状态: 200                               │   │
│  │            │                                             │   │
│  │  10:05:03 ─●─ LLM 调用 (AI 总结)          ⏱️ 11.5s  ◀── │   │
│  │            │  └─ Tokens: 1,234                           │   │
│  │            │                                             │   │
│  │  10:05:15 ─●─ 结束                                       │   │
│  │                                                          │   │
│  └──────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌─ 节点详情: LLM 调用 ─────────────────────────────────────┐   │
│  │                                                          │   │
│  │  📥 输入                          📤 输出                │   │
│  │  ┌────────────────────────┐      ┌────────────────────┐ │   │
│  │  │ {                      │      │ {                  │ │   │
│  │  │   "model": "gpt-4",    │      │   "text": "今天... │ │   │
│  │  │   "prompt": "请总结...",│      │   "tokens": 1234   │ │   │
│  │  │   "temperature": 0.7   │      │ }                  │ │   │
│  │  │ }                      │      │                    │ │   │
│  │  └────────────────────────┘      └────────────────────┘ │   │
│  │                                                          │   │
│  │  [🔄 重新运行此节点]  [📝 修改输入重跑]                   │   │
│  │                                                          │   │
│  └──────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 4.4 数据结构

```typescript
// 执行快照
interface ExecutionSnapshot {
  executionId: string;
  workflowId: string;
  status: ExecutionStatus;
  startedAt: Date;
  completedAt?: Date;
  
  // 节点执行详情
  nodeSnapshots: Map<string, NodeSnapshot>;
  
  // 全局变量
  variables: Record<string, any>;
}

interface NodeSnapshot {
  nodeId: string;
  nodeName: string;
  nodeType: string;
  status: NodeStatus;
  
  // 时间信息
  startedAt: Date;
  completedAt?: Date;
  durationMs: number;
  
  // 数据快照
  inputs: Record<string, any>;
  outputs: Record<string, any>;
  
  // 错误信息
  error?: {
    message: string;
    stack?: string;
  };
  
  // 元数据
  metadata?: {
    tokensUsed?: number;
    model?: string;
    httpStatusCode?: number;
  };
}
```

### 4.5 实现要点

1. **存储优化**: 大型输出数据可压缩存储
2. **增量记录**: 只记录变化的数据
3. **清理策略**: 自动清理旧的执行记录
4. **隐私保护**: 敏感数据可选择不记录

---

## 5. AI 辅助构建

### 5.1 概述

通过自然语言描述，让 AI 自动生成工作流。

### 5.2 功能需求

#### REQ-501: 对话式构建

| ID | 功能 | 优先级 | 说明 |
|----|------|--------|------|
| REQ-501-1 | 意图理解 | P0 | 理解用户想要的工作流 |
| REQ-501-2 | 节点生成 | P0 | 自动生成合适的节点 |
| REQ-501-3 | 连线生成 | P0 | 自动连接节点 |
| REQ-501-4 | 配置填充 | P1 | 自动填充节点配置 |
| REQ-501-5 | 迭代优化 | P1 | 根据反馈修改 |

#### REQ-502: 智能建议

| ID | 功能 | 优先级 | 说明 |
|----|------|--------|------|
| REQ-502-1 | 下一步建议 | P1 | 建议添加什么节点 |
| REQ-502-2 | 配置建议 | P1 | 根据上下文建议配置 |
| REQ-502-3 | 错误修复建议 | P2 | 执行失败时建议修复 |

### 5.3 UI 设计

```
┌─────────────────────────────────────────────────────────────────┐
│                     AI 助手                          [关闭] ✕   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  💬 你想创建什么样的工作流？                             │   │
│  │                                                          │   │
│  │  用户: 帮我创建一个工作流，每天早上获取 Hacker News      │   │
│  │       热门文章，然后用 AI 总结成中文发到我的邮箱          │   │
│  │                                                          │   │
│  │  AI: 好的，我来帮你创建这个工作流。包含以下步骤：        │   │
│  │                                                          │   │
│  │      1. 🕐 定时触发 - 每天 8:00                         │   │
│  │      2. 🌐 HTTP 请求 - 获取 HN 热门文章                 │   │
│  │      3. 🤖 LLM 调用 - 总结文章为中文                    │   │
│  │      4. 📧 发送邮件 - 发送到指定邮箱                    │   │
│  │                                                          │   │
│  │      [✅ 生成工作流]  [🔄 重新生成]                      │   │
│  │                                                          │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  继续对话...                                      [发送] │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 5.4 Prompt 设计

```typescript
const WORKFLOW_GENERATION_PROMPT = `你是 AgentFlow 工作流生成助手。根据用户描述生成工作流定义。

可用的节点类型:
- start: 开始节点
- end: 结束节点
- llm: LLM 调用 (支持 GPT-4, Claude, 本地模型)
- http: HTTP 请求
- template: 文本模板
- condition: 条件判断
- loop: 循环
- variable: 变量
- delay: 延迟

输出格式 (JSON):
{
  "name": "工作流名称",
  "description": "工作流描述",
  "nodes": [
    {
      "id": "node_1",
      "type": "start",
      "position": { "x": 100, "y": 100 },
      "data": { "label": "开始" }
    },
    // ... 更多节点
  ],
  "edges": [
    { "id": "edge_1", "source": "node_1", "target": "node_2" },
    // ... 更多连线
  ]
}

注意:
1. 节点位置要合理布局，从左到右排列
2. 每个节点之间水平间距约 200px
3. 必须包含 start 和 end 节点
4. 节点配置要完整合理

用户需求: {{user_input}}

请生成工作流定义:`;
```

### 5.5 生成流程

```
用户输入 → 意图理解 → 节点规划 → JSON 生成 → 验证 → 渲染到画布
    │          │          │          │        │
    │          │          │          │        └── 检查节点连接
    │          │          │          └── 生成完整定义
    │          │          └── 确定需要哪些节点
    │          └── 提取关键信息
    └── 自然语言描述
```

---

## 6. 其他功能

### 6.1 节点分组 (REQ-601)

**功能**: 将多个节点组合成一个分组，提高复杂工作流的可读性。

```
┌─────────────────────────────────────────┐
│ 📁 数据获取 (折叠)           [展开 ▼]  │
│                                         │
│   包含 3 个节点                          │
│   HTTP → 解析 → 过滤                    │
│                                         │
└─────────────────────────────────────────┘
```

**功能点**:
- 创建分组
- 命名分组
- 折叠/展开
- 分组移动
- 分组删除

### 6.2 工作流版本历史 (REQ-602)

**功能**: 保存工作流的历史版本，支持回滚。

| 功能 | 说明 |
|------|------|
| 自动版本 | 每次保存创建新版本 |
| 版本列表 | 查看历史版本 |
| 版本对比 | 对比两个版本差异 |
| 版本回滚 | 恢复到历史版本 |

### 6.3 连线动画 (REQ-603)

**功能**: 执行时连线显示数据流动动画。

**效果**:
- 执行中的连线显示流动粒子
- 数据传输时闪烁效果
- 不同数据类型不同颜色

---

## 7. 技术架构

### 7.1 桌面应用架构

```
┌─────────────────────────────────────────────────────────────────┐
│                      AgentFlow Desktop                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌────────────────────────────────────────────────────────┐    │
│  │                 Frontend (WebView)                      │    │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌────────┐ │    │
│  │  │ Editor   │  │ Settings │  │ Debugger │  │ AI助手 │ │    │
│  │  └──────────┘  └──────────┘  └──────────┘  └────────┘ │    │
│  └────────────────────────┬───────────────────────────────┘    │
│                           │ Tauri IPC                           │
│  ┌────────────────────────┴───────────────────────────────┐    │
│  │                    Tauri Core (Rust)                    │    │
│  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐ │    │
│  │  │ Command      │  │ Event        │  │ State        │ │    │
│  │  │ Handlers     │  │ System       │  │ Management   │ │    │
│  │  └──────────────┘  └──────────────┘  └──────────────┘ │    │
│  │                           │                             │    │
│  │  ┌────────────────────────┴───────────────────────┐   │    │
│  │  │              Execution Engine (Go)              │   │    │
│  │  │  ┌──────────┐  ┌──────────┐  ┌──────────┐     │   │    │
│  │  │  │ Workflow │  │ Node     │  │ Variable │     │   │    │
│  │  │  │ Parser   │  │ Executor │  │ Store    │     │   │    │
│  │  │  └──────────┘  └──────────┘  └──────────┘     │   │    │
│  │  └────────────────────────────────────────────────┘   │    │
│  │                           │                             │    │
│  │  ┌────────────────────────┴───────────────────────┐   │    │
│  │  │               Local Services                    │   │    │
│  │  │  ┌──────────┐  ┌──────────┐  ┌──────────┐     │   │    │
│  │  │  │ SQLite   │  │ Ollama   │  │ File     │     │   │    │
│  │  │  │ Database │  │ Client   │  │ System   │     │   │    │
│  │  │  └──────────┘  └──────────┘  └──────────┘     │   │    │
│  │  └────────────────────────────────────────────────┘   │    │
│  └─────────────────────────────────────────────────────────┘    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 7.2 技术选型

| 组件 | 技术 | 说明 |
|------|------|------|
| 桌面框架 | Tauri 2.0 | Rust + WebView |
| 前端 | React + TypeScript | 复用 Web 版 |
| 本地执行 | Go (编译为库) | 执行引擎 |
| 本地数据库 | SQLite | 工作流和执行记录 |
| LLM 集成 | Ollama API | 本地模型 |
| 状态管理 | Zustand | 复用 Web 版 |

### 7.3 构建流程

```bash
# 开发
pnpm tauri dev

# 构建
pnpm tauri build

# 输出
# - Windows: .msi, .exe
# - macOS: .dmg, .app
# - Linux: .deb, .AppImage
```

---

## 8. 开发计划

### 8.1 Week 1-2: Tauri 基础

| 任务 | 预估 | 状态 |
|------|------|------|
| Tauri 项目初始化 | 2d | ⬜ |
| 前端代码适配 | 2d | ⬜ |
| IPC 通信设计 | 2d | ⬜ |
| SQLite 集成 | 2d | ⬜ |
| 基础命令实现 | 2d | ⬜ |

### 8.2 Week 3-4: 本地执行引擎

| 任务 | 预估 | 状态 |
|------|------|------|
| Go 执行引擎移植 | 3d | ⬜ |
| Tauri-Go 桥接 | 2d | ⬜ |
| Ollama 集成 | 3d | ⬜ |
| 本地模型选择器 | 2d | ⬜ |

### 8.3 Week 5-6: 时间旅行调试

| 任务 | 预估 | 状态 |
|------|------|------|
| 执行快照存储 | 2d | ⬜ |
| 时间线 UI | 3d | ⬜ |
| 节点详情面板 | 2d | ⬜ |
| 重跑功能 | 3d | ⬜ |

### 8.4 Week 7-8: AI 辅助 & 打磨

| 任务 | 预估 | 状态 |
|------|------|------|
| AI 生成 Prompt | 2d | ⬜ |
| 生成 UI | 2d | ⬜ |
| 节点分组 | 2d | ⬜ |
| 应用打包 | 2d | ⬜ |
| 测试和修复 | 2d | ⬜ |

---

## 附录

### A. Ollama 安装指南

**macOS**:
```bash
brew install ollama
ollama serve
ollama pull llama3.1:8b
```

**Windows**:
1. 下载 https://ollama.ai/download
2. 安装并运行
3. `ollama pull llama3.1:8b`

**Linux**:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull llama3.1:8b
```

### B. 变更记录

| 版本 | 日期 | 变更内容 | 作者 |
|------|------|----------|------|
| v1.0 | 2026-01-29 | 初始版本 | - |
